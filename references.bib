@Article{Avery2019,
  author          = {Patrick Avery and Xiaoyu Wang and Corey Oses and Eric Gossett and Davide M. Proserpio and Cormac Toher and Stefano Curtarolo and Eva Zurek},
  journal         = {npj Computational Materials},
  title           = {Predicting superhard materials via a machine learning informed evolutionary structure search},
  year            = {2019},
  issn            = {20573960},
  month           = {sep},
  number          = {1},
  volume          = {5},
  abstract        = {The computational prediction of superhard materials would enable the in silico design of compounds that could be used in a wide variety of technological applications. Herein, good agreement was found between experimental Vickers hardnesses, Hv, of a wide range of materials and those calculated by three macroscopic hardness models that employ the shear and/or bulk moduli obtained from: (i) first principles via AFLOW-AEL (AFLOW Automatic Elastic Library), and (ii) a machine learning (ML) model trained on materials within the AFLOW repository. Because HvML values can be quickly estimated, they can be used in conjunction with an evolutionary search to predict stable, superhard materials. This methodology is implemented in the XtalOpt evolutionary algorithm. Each crystal is minimized to the nearest local minimum, and its Vickers hardness is computed via a linear relationship with the shear modulus discovered by Teter. Both the energy/enthalpy and Hv,TeterML are employed to determine a structure's fitness. This implementation is applied towards the carbon system, and 43 new superhard phases are found. A topological analysis reveals that phases estimated to be slightly harder than diamond contain a substantial fraction of diamond and/or lonsdaleite.},
  annote          = {Using material databases with shear and/or bulk modulus to calculate Vickers hardness, and combining that with a macroscopic model and Machine Learning to optimize for the hardness. Optimization is done with evolutionary algorithms (EA) and certain lattice structures having a high hardness and low energy (=stable) are mentioned Good example of reversing the process of materials design},
  archiveprefix   = {arXiv},
  arxivid         = {1906.05886},
  doi             = {10.1038/s41524-019-0226-8},
  eprint          = {1906.05886},
  file            = {:home/martin/Downloads/s41524-019-0226-8.pdf:pdf},
  mendeley-groups = {Data-driven materials design},
  publisher       = {Springer Science and Business Media {LLC}},
  url             = {http://dx.doi.org/10.1038/s41524-019-0226-8},
}

@Article{Yang2019,
  author          = {Kai Yang and Xinyi Xu and Benjamin Yang and Brian Cook and Herbert Ramos and N. M. Anoop Krishnan and Morten M. Smedskjaer and Christian Hoover and Mathieu Bauchy},
  journal         = {Scientific Reports},
  title           = {{Predicting the Young's Modulus of Silicate Glasses using High-Throughput Molecular Dynamics Simulations and Machine Learning}},
  year            = {2019},
  issn            = {20452322},
  month           = {jun},
  number          = {1},
  pages           = {1--11},
  volume          = {9},
  abstract        = {The application of machine learning to predict materials' properties usually requires a large number of consistent data for training. However, experimental datasets of high quality are not always available or self-consistent. Here, as an alternative route, we combine machine learning with high-throughput molecular dynamics simulations to predict the Young's modulus of silicate glasses. We demonstrate that this combined approach offers good and reliable predictions over the entire compositional domain. By comparing the performances of select machine learning algorithms, we discuss the nature of the balance between accuracy, simplicity, and interpretability in machine learning.},
  annote          = {Combining high-throughput MD simulations of silicate glasses as input data for ML models to predict the compisition (x,y) of a CaO-Al2O3-SiO2 system for the Young's Modulus (E) Four ML models are tested: Polynomial regression LASSO, Random Forest and an ANN. ML Models as well as a conventional model (Makishima-Mackenzie) are subjected to experimental data. ML models predict the non-linear behaviour better.},
  doi             = {10.1038/s41598-019-45344-3},
  file            = {:home/martin/Downloads/s41598-019-45344-3.pdf:pdf},
  mendeley-groups = {Data-driven materials design},
  pmid            = {31217500},
  publisher       = {Springer Science and Business Media {LLC}},
  url             = {http://dx.doi.org/10.1038/s41598-019-45344-3},
}

@Article{Mozaffar2019,
  author          = {M. Mozaffar and R. Bostanabad and W. Chen and K. Ehmann and J. Cao and M. A. Bessa},
  journal         = {Proceedings of the National Academy of Sciences},
  title           = {Deep learning predicts path-dependent plasticity},
  year            = {2019},
  issn            = {10916490},
  month           = {dec},
  number          = {52},
  pages           = {26414--26420},
  volume          = {116},
  abstract        = {Plasticity theory aims at describing the yield loci and work hardening of a material under general deformation states. Most of its complexity arises from the nontrivial dependence of the yield loci on the complete strain history of a material and its microstructure. This motivated 3 ingenious simplifications that underpinned a century of developments in this field: 1) yield criteria describing yield loci location; 2) associative or nonassociative flow rules defining the direction of plastic flow; and 3) effective stress-strain laws consistent with the plastic work equivalence principle. However, 2 key complications arise from these simplifications. First, finding equations that describe these 3 assumptions for materials with complex microstructures is not trivial. Second, yield surface evolution needs to be traced iteratively, i.e., through a return mapping algorithm. Here, we show that these assumptions are not needed in the context of sequence learning when using recurrent neural networks, diverting the above-mentioned complications. This work offers an alternative to currently established plasticity formulations by providing the foundations for finding history- and microstructure-dependent constitutive models through deep learning.},
  annote          = {Predicting history-dependent plasticity by using data from FEM simulations as input for a RNN. Using the F3DASM framework, we make a DoE and collect the data ourselves by finite element analysis. Data collection takes a long time, but makes sufficient data to train a RNN, which predicts the behaviour of unseen RVEs.},
  doi             = {10.1073/pnas.1911815116},
  file            = {:home/martin/Downloads/pnas.1911815116.pdf:pdf},
  keywords        = {Data-driven modeling, Deep learning, Plasticity, Recurrent neural network},
  mendeley-groups = {Data-driven materials design},
  pmid            = {31843918},
  publisher       = {Proceedings of the National Academy of Sciences},
}

@Article{Bessa2017,
  author          = {Bessa, M. A. and Bostanabad, R. and Liu, Z. and Hu, A. and Apley, Daniel W. and Brinson, C. and Chen, W. and Liu, Wing Kam},
  journal         = {Computer Methods in Applied Mechanics and Engineering},
  title           = {A framework for data-driven analysis of materials under uncertainty: Countering the curse of dimensionality},
  year            = {2017},
  issn            = {00457825},
  month           = {jun},
  number          = {April},
  pages           = {633--667},
  volume          = {320},
  abstract        = {A new data-driven computational framework is developed to assist in the design and modeling of new material systems and structures. The proposed framework integrates three general steps: (1) design of experiments, where the input variables describing material geometry (microstructure), phase properties and external conditions are sampled; (2) efficient computational analyses of each design sample, leading to the creation of a material response database; and (3) machine learning applied to this database to obtain a new design or response model. In addition, the authors address the longstanding challenge of developing a data-driven approach applicable to problems that involve unacceptable computational expense when solved by standard analysis methods – e.g. finite element analysis of representative volume elements involving plasticity and damage. In these cases the framework includes the recently developed “self-consistent clustering analysis” method in order to build large databases suitable for machine learning. The authors believe that this will open new avenues to finding innovative materials with new capabilities in an era of high-throughput computing (“big-data”).},
  doi             = {10.1016/j.cma.2017.03.037},
  file            = {:home/martin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bessa et al. - 2017 - A framework for data-driven analysis of materials under uncertainty Countering the curse of dimensionality.pdf:pdf},
  keywords        = {Design of experiments, Machine learning and data mining, Plasticity, Reduced order model, Self-consistent clustering analysis},
  mendeley-groups = {MSc Thesis/Application optimalization,Data-driven materials design},
  publisher       = {Elsevier {BV}},
}

@Article{Kadulkar2022,
  author          = {Sanket Kadulkar and Zachary M. Sherman and Venkat Ganesan and Thomas M. Truskett},
  journal         = {Annual Review of Chemical and Biomolecular Engineering},
  title           = {{Machine Learning-Assisted Design of Material Properties}},
  year            = {2022},
  issn            = {1947-5438},
  month           = {mar},
  number          = {1},
  volume          = {13},
  abstract        = {Designing functional materials requires a deep search through multidimensional spaces for system parameters that yield desirable material properties. For cases where conventional parameter sweeps or trial-and-error sampling are impractical, inverse methods that frame design as a constrained optimization problem present an attractive alternative. However, even efficient algorithms require time- and resource-intensive characterization of material properties many times during optimization, imposing a design bottleneck. Approaches that incorporate machine learning can help address this limitation and accelerate the discovery of materials with targeted properties. In this article, we review how to leverage machine learning to reduce dimensionality in order to effectively explore design space, accelerate property evaluation, and generate unconventional material structures with optimal properties. We also discuss promising future directions, including integration of machine learning into multiple stages of a design algorithm and interpretation of machine learning models to understand how design parameters relate to material properties.Expected final online publication date for the Annual Review of Chemical and Biomolecular Engineering, Volume 13 is October 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  archiveprefix   = {arXiv},
  arxivid         = {2201.11168},
  doi             = {10.1146/annurev-chembioeng-092220-024340},
  eprint          = {2201.11168},
  file            = {:home/martin/Downloads/2201.11168.pdf:pdf},
  mendeley-groups = {Data-driven materials design},
  publisher       = {Annual Reviews},
  url             = {http://arxiv.org/abs/2201.11168},
}

@Article{Guo2021,
  author          = {Kai Guo and Zhenze Yang and Chi-Hua Yu and Markus J. Buehler},
  journal         = {Materials Horizons},
  title           = {Artificial intelligence and machine learning in design of mechanical materials},
  year            = {2021},
  issn            = {20516355},
  number          = {4},
  pages           = {1153--1172},
  volume          = {8},
  abstract        = {Artificial intelligence, especially machine learning (ML) and deep learning (DL) algorithms, is becoming an important tool in the fields of materials and mechanical engineering, attributed to its power to predict materials properties, design de novo materials and discover new mechanisms beyond intuitions. As the structural complexity of novel materials soars, the material design problem to optimize mechanical behaviors can involve massive design spaces that are intractable for conventional methods. Addressing this challenge, ML models trained from large material datasets that relate structure, properties and function at multiple hierarchical levels have offered new avenues for fast exploration of the design spaces. The performance of a ML-based materials design approach relies on the collection or generation of a large dataset that is properly preprocessed using the domain knowledge of materials science underlying chemical and physical concepts, and a suitable selection of the applied ML model. Recent breakthroughs in ML techniques have created vast opportunities for not only overcoming long-standing mechanics problems but also for developing unprecedented materials design strategies. In this review, we first present a brief introduction of state-of-the-art ML models, algorithms and structures. Then, we discuss the importance of data collection, generation and preprocessing. The applications in mechanical property prediction, materials design and computational methods using ML-based approaches are summarized, followed by perspectives on opportunities and open challenges in this emerging and exciting field.},
  doi             = {10.1039/d0mh01451f},
  file            = {:home/martin/Downloads/d0mh01451f.pdf:pdf},
  mendeley-groups = {Data-driven materials design},
  pmid            = {34821909},
  publisher       = {Royal Society of Chemistry ({RSC})},
}

@Article{Gongora2020,
  author          = {Aldair E. Gongora and Bowen Xu and Wyatt Perry and Chika Okoye and Patrick Riley and Kristofer G. Reyes and Elise F. Morgan and Keith A. Brown},
  journal         = {Science Advances},
  title           = {{A Bayesian experimental autonomous researcher for mechanical design}},
  year            = {2020},
  issn            = {23752548},
  month           = {apr},
  number          = {15},
  volume          = {6},
  abstract        = {While additive manufacturing (AM) has facilitated the production of complex structures, it has also highlighted the immense challenge inherent in identifying the optimum AM structure for a given application. Numerical methods are important tools for optimization, but experiment remains the gold standard for studying nonlinear, but critical, mechanical properties such as toughness. To address the vastness of AM design space and the need for experiment, we develop a Bayesian experimental autonomous researcher (BEAR) that combines Bayesian optimization and high-throughput automated experimentation. In addition to rapidly performing experiments, the BEAR leverages iterative experimentation by selecting experiments based on all available results. Using the BEAR, we explore the toughness of a parametric family of structures and observe an almost 60-fold reduction in the number of experiments needed to identify high-performing structures relative to a grid-based search. These results show the value of machine learning in experimental fields where data are sparse.},
  doi             = {10.1126/sciadv.aaz1708},
  file            = {:home/martin/Downloads/sciadv.aaz1708.pdf:pdf},
  mendeley-groups = {Data-driven materials design},
  pmid            = {32300652},
  publisher       = {American Association for the Advancement of Science ({AAAS})},
}

@InProceedings{Yogatama2014,
  author       = {Yogatama, Dani and Mann, Gideon},
  booktitle    = {Artificial intelligence and statistics},
  title        = {Efficient transfer learning method for automatic hyperparameter tuning},
  year         = {2014},
  organization = {PMLR},
  pages        = {1077--1085},
}

@InProceedings{Golovin2017,
  author    = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, David},
  booktitle = {Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  title     = {Google vizier: A service for black-box optimization},
  year      = {2017},
  pages     = {1487--1495},
}

@Article{Ma2019,
  author    = {Ma, Yi-An and Chen, Yuansi and Jin, Chi and Flammarion, Nicolas and Jordan, Michael I},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Sampling can be faster than optimization},
  year      = {2019},
  number    = {42},
  pages     = {20881--20885},
  volume    = {116},
  publisher = {National Acad Sciences},
}

@Article{Wistuba2018,
  author    = {Wistuba, Martin and Schilling, Nicolas and Schmidt-Thieme, Lars},
  journal   = {Machine Learning},
  title     = {Scalable gaussian process-based transfer surrogates for hyperparameter optimization},
  year      = {2018},
  number    = {1},
  pages     = {43--78},
  volume    = {107},
  publisher = {Springer},
}

@Article{Vanschoren2018,
  author  = {Vanschoren, Joaquin},
  journal = {arXiv preprint arXiv:1810.03548},
  title   = {Meta-learning: A survey},
  year    = {2018},
}

@Article{Li2016,
  author  = {Li, Ke and Malik, Jitendra},
  journal = {arXiv preprint arXiv:1606.01885},
  title   = {Learning to optimize},
  year    = {2016},
}

@InProceedings{Thornton2013,
  author    = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
  booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  title     = {Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms},
  year      = {2013},
  pages     = {847--855},
}

@InProceedings{Domhan2015,
  author    = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  booktitle = {Twenty-fourth international joint conference on artificial intelligence},
  title     = {Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  year      = {2015},
}

@Article{Swersky2014,
  author  = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  journal = {arXiv preprint arXiv:1406.3896},
  title   = {Freeze-thaw Bayesian optimization},
  year    = {2014},
}

@Article{Wang2020,
  author    = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
  journal   = {ACM computing surveys (csur)},
  title     = {Generalizing from a few examples: A survey on few-shot learning},
  year      = {2020},
  number    = {3},
  pages     = {1--34},
  volume    = {53},
  publisher = {ACM New York, NY, USA},
}

@Article{Hospedales2020,
  author  = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal = {arXiv preprint arXiv:2004.05439},
  title   = {Meta-learning in neural networks: A survey},
  year    = {2020},
}

@Article{Flennerhag2018,
  author  = {Flennerhag, Sebastian and Moreno, Pablo G and Lawrence, Neil D and Damianou, Andreas},
  journal = {arXiv preprint arXiv:1812.01054},
  title   = {Transferring knowledge across learning processes},
  year    = {2018},
}

@InProceedings{Bechtle2021,
  author       = {Bechtle, Sarah and Molchanov, Artem and Chebotar, Yevgen and Grefenstette, Edward and Righetti, Ludovic and Sukhatme, Gaurav and Meier, Franziska},
  booktitle    = {2020 25th International Conference on Pattern Recognition (ICPR)},
  title        = {Meta learning via learned loss},
  year         = {2021},
  organization = {IEEE},
  pages        = {4161--4168},
}

@Article{Nichol2018,
  author  = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  journal = {arXiv preprint arXiv:1803.02999},
  title   = {On first-order meta-learning algorithms},
  year    = {2018},
}

@Article{Andrychowicz2016,
  author  = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal = {Advances in neural information processing systems},
  title   = {Learning to learn by gradient descent by gradient descent},
  year    = {2016},
  volume  = {29},
}

@InProceedings{Finn2017,
  author       = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle    = {International conference on machine learning},
  title        = {Model-agnostic meta-learning for fast adaptation of deep networks},
  year         = {2017},
  organization = {PMLR},
  pages        = {1126--1135},
}

@Article{Duan2016,
  author  = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1611.02779},
  title   = {Rl $\^{} 2$: Fast reinforcement learning via slow reinforcement learning},
  year    = {2016},
}

@Article{Cao2019,
  author  = {Cao, Yue and Chen, Tianlong and Wang, Zhangyang and Shen, Yang},
  journal = {Advances in neural information processing systems},
  title   = {Learning to optimize in swarms},
  year    = {2019},
  volume  = {32},
}

@Article{Wang2016,
  author  = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  journal = {arXiv preprint arXiv:1611.05763},
  title   = {Learning to reinforcement learn},
  year    = {2016},
}

@MastersThesis{Schelling2021,
  author = {van der Schelling, Martin},
  school = {Delft University of Technology},
  title  = {A data-driven heuristic decision strategy for data-scarce optimization: with an application towards bio-based composites},
  year   = {2021},
  month  = mar,
  type   = {mathesis},
  url    = {https://repository.tudelft.nl/islandora/object/uuid:d58271d6-21bb-470c-a5ee-4584b3b8ee29?collection=education},
}

@Article{Wolpert1997,
  author    = {Wolpert, David H. and Macready, William G.},
  journal   = {IEEE transactions on evolutionary computation},
  title     = {No free lunch theorems for optimization},
  year      = {1997},
  number    = {1},
  pages     = {67--82},
  volume    = {1},
  publisher = {IEEE},
}

@Article{Bessa2019,
  author    = {Bessa, Miguel A. and Glowacki, Piotr and Houlder, Michael},
  journal   = {Advanced Materials},
  title     = {Bayesian machine learning in metamaterial design: Fragile becomes supercompressible},
  year      = {2019},
  number    = {48},
  pages     = {1904845},
  volume    = {31},
  publisher = {Wiley Online Library},
}

@Comment{jabref-meta: databaseType:bibtex;}
